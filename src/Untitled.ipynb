{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath = path + '\\\\res\\datasets\\\\riloff_hn\\\\'\n",
    "train_sample = 'train_hn_sample.txt'\n",
    "test_sample = 'test_hn_sample.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load file\n",
    "file = open(datapath+train_sample , 'r',encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sample = file.split('\\n')\n",
    "data_sample# = sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokenized(data_sample):\n",
    "    tokenized = [sen.split(' ') for sen in data_sample]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsp = get_tokenized(data_sample)\n",
    "sample = tsp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test tokenization\n",
    "for sen in tokenized_data_sample:\n",
    "    print(len(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removehastags(tokenizedtweet):\n",
    "    for token in tokenizedtweet:\n",
    "        if(token[0]=='#'):\n",
    "            if(len(token)==1):\n",
    "                #reomve nexttoken\n",
    "            else:\n",
    "                #reomve token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HindiTokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample ='निह मिन बॉट प्रशंसकों शस्त्रागार @my_supersoccer: डी http://t.co/6psFCPza'\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_mention(toksen):\n",
    "    for n, tok in enumerate(toksen):\n",
    "        if tok[0]=='@':\n",
    "            if len(tok)>1:\n",
    "                toksen[n] = '@उपयोगकर्ता'\n",
    "            else:\n",
    "                print(\"got here\")\n",
    "                #toksen.remove(n+1)\n",
    "                toksen[n] = '@उपयोगकर्ता'\n",
    "    return toksen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = replace_mention(sample)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_hastag(toksen):\n",
    "    for n, tok in enumerate(toksen):\n",
    "        if tok[0]=='#':\n",
    "            if len(tok)>1:\n",
    "                print(\"found\")\n",
    "                toksen.pop(n)\n",
    "            else:\n",
    "                print(\"found again\")\n",
    "                toksen.pop(n)\n",
    "                toksen.pop(n)\n",
    "    return toksen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(remove_hastag(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = re.sub('([!?*&%\"~`^+{}])', r' \\1 ', sample)\n",
    "tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = ['176.mp3', '197.mp3', '24.mp3', '263.mp3', '272.mp3', '276.mp3', '282.mp3', '292.mp3', '30.mp3', '304.mp3', '308.mp3', '38.mp3', '39.mp3', '41.mp3', '44.mp3', '45.mp3', '51.mp3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = [i[:-4] for i in st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = [int(i) for i in st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st =sorted(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in st:\n",
    "    print(str(i)+\".mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output list of all files in current directory\n",
    "import os\n",
    "import re\n",
    "#import emoji\n",
    "files = os.listdir(os.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path = '\\\\'.join((path).split('\\\\')[:-1])\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "datapath = path + \"\\\\res\\\\datasets\\\\\"\n",
    "train = \"train_hn.txt\"\n",
    "test = \"test_hn.txt\"\n",
    "dataset = \"\\\\riloff_hn\\\\\"\n",
    "train_filename = utils.load_file( datapath + dataset + train)\n",
    "test_filename = utils.load_file( datapath + dataset + test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_filename) #length of train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = train_filename\n",
    "tokens_path = path + \"\\\\res\\\\tokens\\\\\"\n",
    "utils.save_file(lines, tokens_path + \"tokens_original_\"+ train)\n",
    "utils.save_file(test_filename, tokens_path + \"tokens_original_\"+ test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#characters level vocabulary\n",
    "vocabchar ={}\n",
    "for line in lines[:10]:\n",
    "    word_list = list(line)\n",
    "    for word in word_list:\n",
    "        if word not in vocabchar:\n",
    "            vocabchar[word]=1\n",
    "        else:\n",
    "            vocabchar[word]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word level vocabulary\n",
    "def vocab_build(lines):\n",
    "    vocabulary ={}\n",
    "    for line in lines:\n",
    "        #tokenize sentences\n",
    "        tweet = re.sub('([!?*&%\"~`^+{}])', r' \\1 ', line)\n",
    "        tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "        words = tweet.split()\n",
    "        #print((word_list))\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word]=1\n",
    "            else:\n",
    "                vocabulary[word]+=1\n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(vocabulary) = vocab_build(lines)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_list = utils.load_file(path+ \"\\\\res\\\\word_list.txt\")\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w=set()\n",
    "for word in word_list:\n",
    "    w.add(word)\n",
    "w =list(w)\n",
    "len(w)\n",
    "#w ='\\n'.join(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utils.save_file(w,path+ \"\\\\res\\\\word_list.txt\")\n",
    "#filtering repeated words in word_list() #220525\n",
    "word_list = utils.load_file(path+ \"\\\\res\\\\word_list.txt\")\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list_freq={word : 0 for word in word_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dictionary_build(dictionary, lines):\n",
    "    for line in lines:\n",
    "        tweet = re.sub('([!?*&%\"~`^+{}])', r' \\1 ', line)\n",
    "        tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "        words = tweet.split()\n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                dictionary[word]+=1\n",
    "            else:\n",
    "                dictionary[word]=1\n",
    "    return dictionary\n",
    "d = dictionary_build(word_list_freq, lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet, remove_hashtag=True, remove_emojis=False, remove_user_mentions =False, remove_eng= True):\n",
    "    tweet = re.sub('([!?*&%\"~`^+{}])', r' \\1 ', tweet)\n",
    "    tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "    valid_tokens =[]\n",
    "    for word in tweet.split():\n",
    "        #remove sarcasm hashtag\n",
    "        if word.startswith(\"#sarca\"):\n",
    "            continue\n",
    "        if 'http' in word.lower():\n",
    "            continue\n",
    "        if remove_hashtag and word.startswith(\"#\"):\n",
    "            continue\n",
    "        if remove_user_mentions and word.startswith(\"@\"):\n",
    "            continue\n",
    "        if remove_user_mentions==False and word.startswith('@'):\n",
    "            valid_tokens.append('@उपयोगकर्ता')\n",
    "            continue\n",
    "        if remove_eng and f_eng(word):\n",
    "            continue\n",
    "        #if remove_emojis and word in emoji.UNICODE_EMOJI:\n",
    "         #   continue\n",
    "        valid_tokens.append(word)\n",
    "    return ' '.join(valid_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clean_tweet(tweets):\n",
    "    cleaned_tweets =[]\n",
    "    for tweet in tweets:\n",
    "        clean_tw = clean_tweet(tweet)\n",
    "        cleaned_tweets.append(clean_tw)\n",
    "    return cleaned_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl_tws_train = get_clean_tweet(train_filename)\n",
    "cl_tws_test = get_clean_tweet(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl_tws_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.save_file(cl_tws_train, tokens_path + \"tokens_\"+\"clean_\"+\"original_\"+ train)\n",
    "utils.save_file(cl_tws_test, tokens_path + \"tokens_\"+\"clean_\"+\"original_\"+ test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_eng(word):\n",
    "    pattern = re.search('[A-Za-z]', word)\n",
    "    return (pattern!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = 'lmfao Cinnabon मेरे पीछे आ गया'\n",
    "tweet = re.sub('([!?*&%\"~`^+{}])', r' \\1 ', tweet)\n",
    "tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "for tw in tweet.split():\n",
    "    print(f_eng(tw))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ENGLISH_CHARS = re.compile('[\\w]', re.IGNORECASE)\n",
    "ALL_CHARS = re.compile('[^\\W_]', re.IGNORECASE | re.UNICODE)\n",
    " \n",
    "print( (ENGLISH_CHARS))\n",
    "#assert len(ALL_CHARS.findall('_àÖÎ_')) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string =\"7_मेरे\"\n",
    "pattern = re.search('[A-Za-z]', string)\n",
    "print(pattern==None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_user_mention(tweet):\n",
    "    tweet = re.sub('([!?*&%\"~`^+{}])', r' \\1 ', tweet)\n",
    "    tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "    for tw in tweet.split():\n",
    "        if tw.startswith('@'):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st ='ASDD'\n",
    "print(st.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  baseline feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_len_info(tweets, average=False):\n",
    "    sum_of_length = sum([len(l.split()) for l in tweets])\n",
    "    avg_tweet_len = sum_of_length / float(len(tweets))\n",
    "    print(\"Mean of train tweets: \", avg_tweet_len)\n",
    "    max_tweet_len = len(max(tweets, key=len).split())\n",
    "    print(\"Max tweet length is = \", max_tweet_len)\n",
    "    if average:\n",
    "        return avg_tweet_len\n",
    "    return max_tweet_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_max_len_info(train_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = Tokenizer(num_words =35, filters ='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=lower, split= \" \", char_level = char_level )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e0ce13165e25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#from gensim import corpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "#from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
